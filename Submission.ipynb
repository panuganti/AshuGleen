{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "- Problem Statement\n",
    "- Given an open-source model, Execute the application\n",
    "- Discussion on the selection of the model and the algorithm\n",
    "- Discussion on Evaluation\n",
    "- Fine-tuning the model\n",
    "- Closing Remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CheatApp\n",
    "\n",
    "We are going to develop a cheating app for open domain question answering systems through a notebook. In this app, we would like to suggest users of the wikipedia page with the relevant answers for given questions. To further stretch the challenge, we would like to suggest the best paragraphs having the answers of the questions in the corresponding wikipedia page. Below are few examples - \n",
    "\n",
    "Question:  how are glacier caves formed ?\n",
    "wikipedia page - Glacier cave - Wikipedia   \n",
    "paragraph : ‘A glacier cave is a cave formed within the ice of a glacier. Glacier caves are often called ice caves, but the latter term is properly used to describe bedrock caves that contain year-round ice’ (summary of the page). \n",
    "\n",
    "Question - how much is 1 tablespoon of water ?\n",
    "wikipedia page -https://en.wikipedia.org/wiki/Tablespoon  \n",
    "paragraph is - It has multiple answers. It could like - \n",
    "‘In most places, except Australia, one tablespoon equals three teaspoons—and one US tablespoon is 14.8 ml (0.50 US fl oz; 0.52 imp fl oz) or 15 ml (0.51 US fl oz; 0.53 imp fl oz).’ \n",
    "Or\n",
    " ‘In nutrition labeling in the U.S. and the U.K., a tablespoon is defined as 15 ml (0.51 US fl oz).[7] In Australia, the definition of the tablespoon is 20 ml (0.70 imp fl oz)’ etc.\n",
    "\n",
    "Question - how did anne frank die \n",
    "wikipedia page - https://en.wikipedia.org/wiki/Anne_Frank \n",
    "Paragraph - ‘Following their arrest, the Franks were transported to concentration camps. On 1 November 1944,[2] Anne and her sister, Margot, were transferred from Auschwitz to Bergen-Belsen concentration camp, where they died (probably of typhus) a few months later. They were originally estimated by the Red Cross to have died in March, with Dutch authorities setting 31 March as the official date. Later research has suggested they died in February or early March.’\n",
    "\n",
    "Expectation\n",
    "Given this is an open problem, we don’t expect a particular level of correctness. What we are mainly looking for - how you approach and quickly prototype crappy solutions. Then you keep adding complex logic in iterations to achieve some satisfactory levels. While doing that journey, we expect that you may generate following artifacts - \n",
    "Hypothesis and motivations for choosing different modeling techniques.\n",
    "How you measured the model performance. \n",
    "Data curation, training/evaluation data generations, model performance measurements etc.\n",
    "end 2 end machine learning pipeline in python notebook including above steps.\n",
    "Also, what constraints you felt which led you not to try the things you wanted to do to solve this problem is an awesome way.\n",
    "** -  If you use an already available model/code/library from the web, we expect that you have a full understanding of motivation and why you are using it. Ex:- if you use entity linking library, we expect that you understand - pros and cons of that model. This includes - Why do you think your chosen entity linking library is good for your problem?  When do you expect your chosen model may behave poorly? \n",
    "\n",
    "Resources\n",
    "\n",
    "You are free to use open source resources including already available  annotated training data on the web. Also, free to use already trained models & libraries existing in open source. What we mainly expect is - how you approach the problems and journey.\n",
    "\n",
    "You are not allowed to use llm libraries like Langchain and LammaIndex. \n",
    "\n",
    "Wikipedia text data is available in Kaggle at - wikidata-text\n",
    "Also added sample open questions and expected answers - wikipedia_question_similar_answer.tsv . The answers added here are not exact wikipedia graphs, but it may be super helpful for your modeling techniques. \n",
    "\n",
    "Other open source resources that can be used are - https://paperswithcode.com/dataset/wikiqa (questions in wikipedia_question_similar_answer.tsv is taken from this data set).\n",
    "\n",
    "\n",
    "\n",
    "Notes\n",
    "Please create a loom video explaining all solutions/approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Solution\n",
    "The problem corresponds to the Question-Answering problem of the NLP domain.\n",
    "Inputs: Query, a set of wiki-urls\n",
    "Output: Answer with citations (Let's limit to 2 (configurable) for brevity)\n",
    "\n",
    "Algorithm:\n",
    "Step 1: Find the most relevant url. <br>\n",
    "There can be multiple url's satisfying a question. However, for simplicity, I assumed only one url has the relevant answer.\n",
    "Please note that there is nothing in the algorithm that breaks if there are more than 1 url that can contain the answer\n",
    "The algorithm we use for this case is the cosine similarity score to rank the urls as per relevance to the question.\n",
    "\n",
    "For the computation of these embeddings, we will use the model hugging-face's **paraphrase-MiniLM-L6-v2**.\n",
    "We compute the embedding for the question and also the embedding for the entire text in the wiki url.\n",
    "Then, we sort the urls based on the cosine similarity between the question and the text of the wiki url.\n",
    "\n",
    "**Why we chose this model**\n",
    "Wiki text can be particularly long context since the answer can be present anywhere in the text. \n",
    "**paraphrase-MiniLM-L6-v2** is a model that can handle a faily long text in the context. I have experimented with other models like **distilbert-base-uncased**.\n",
    "They are however unable to generate a good embedding for a long context. \n",
    "The quality of this model can also be observed in the tests that I have executed to test this algorithm. See the output of the next Code cell.\n",
    "\n",
    "**pros**\n",
    "1. Good quality\n",
    "2. Long Context\n",
    "\n",
    "**cons**\n",
    "1. Higher latency\n",
    "2. Not perfect. It needs to be tuned further.\n",
    "\n",
    "Step 2: Find the most relevant paragraph within the url. <br>\n",
    "We first split the text into separate paragraphs.\n",
    "We compute the embeddings for each of these paragrahs and the top-2 most relevant paragraphs are chosen again based on the cosine similarity.\n",
    "For this step too, we choose the same model. In this step, ideally, we can chose a much lightweight model including **distilbert-base-uncased**.\n",
    "The performance of **distilbert-base-uncased** model is also reasonably satisfactory for such smaller context. Even though, the **paraphrase-MiniLM-L6-v2** performs slightly better.\n",
    "\n",
    "Step 3: From the most relevant paragraphs, generate the answer. <br>\n",
    "Now that we found the most relevant paragraphs that might contain the answer, it is important to extract the answer.\n",
    "For this, we use HuggingFace's **text2text-generation** pipeline with **t5-base**. The model performs particularly well especially in short contexts.\n",
    "There are better models than **t5-base** but this model is sufficient for the length of context we are supplying. In addition, it is of lower latency.\n",
    "\n",
    "The reason why Step 3 is important is that we need to determine if there is present at all or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how are glacier caves formed\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
      "Answer: by water running through or under the glacier\n",
      "Paragraph: Most glacier caves are started by water running through or under the glacier. This water often originates on the glacier's surface through melting, entering the ice at a moulin and exiting at the glacier's snout at base level. Heat transfer from the water can cause sufficient melting to create an air-filled cavity, sometimes aided by solifluction. Air movement can then assist enlargement through melting in summer and sublimation in winter.\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
      "Answer: geothermal heat from volcanic vents or hotsprings beneath the ice\n",
      "Paragraph: Some glacier caves are formed by geothermal heat from volcanic vents or hotsprings beneath the ice.  An extreme example is the Kverkfjöll glacier cave in the Vatnajökull glacier in Iceland, measured in the 1980s at 2.8 kilometres (1.7 mi) long with a vertical range of 525 metres (1,722 ft).\n",
      "\n",
      "Question: how much is 1 tablespoon of water ?\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
      "Answer: three teaspoons\n",
      "Paragraph: In most places, except Australia, one tablespoon equals three teaspoons—and one US tablespoon is 14.8 ml (0.50 US fl oz; 0.52 imp fl oz) or 15 ml (0.51 US fl oz; 0.53 imp fl oz).\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
      "Answer: 15 ml (0.51 US fl oz)\n",
      "Paragraph: In nutrition labeling in the U.S. and the U.K., a tablespoon is defined as 15 ml (0.51 US fl oz).[7] In Australia, the definition of the tablespoon is 20 ml (0.70 imp fl oz).[citation needed]\n",
      "\n",
      "Question: how did anne frank die\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Anne_Frank\n",
      "Answer: a typhus epidemic\n",
      "Paragraph: Anne Frank died at the Bergen-Belsen concentration camp in February or March 1945. The specific cause is unknown; however, there is evidence to suggest that she died from a typhus epidemic that spread through the camp, killing 17,000 prisoners.[98] Gena Turgel, a survivor of Bergen-Belsen, knew Anne at the camp. In 2015, she told the British newspaper The Sun: \"Her bed was around the corner from me. She was delirious, terrible, burning up.\" She said she had brought Frank water to wash.[99] Turgel, who worked in the camp hospital, said that the epidemic took a terrible toll on the inmates: \"The people were dying like flies—in the hundreds. Reports used to come in—500 people who died. Three hundred? We said, 'Thank God, only 300.'\"[99] Other diseases, including typhoid fever, were rampant.[100]\n",
      "\n",
      "Question: how a water pump works\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
      "Answer: irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor\n",
      "Paragraph: Pumps are used throughout society for a variety of purposes.  Early applications includes the use of  the windmill or watermill to pump water.  Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
      "Answer: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the\n",
      "Paragraph: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.\n",
      "\n",
      "Question: how old was sue lyon when she made lolita\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Sue_Lyon\n",
      "Answer: 12\n",
      "Paragraph: Although Vladimir Nabokov originally thought that Sue Lyon was the right selection to play Lolita, years later Nabokov said that the ideal Lolita would have been Catherine Demongeot, a young French actress who had played the child Zazie in Louis Malle's Zazie in the Metro (1960). The tomboyish Demongeot was four years younger than Lyon.[12]\n",
      "\n",
      "Question: how are fire bricks made\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Fire_brick\n",
      "Answer: ceramic material\n",
      "Paragraph: A fire brick, firebrick, fireclay brick, or refractory brick is a block of ceramic material used in lining furnaces, kilns, fireboxes, and fireplaces.  A refractory brick is built primarily to withstand high temperature, but will also usually have a low thermal conductivity for greater energy efficiency. Usually dense fire bricks are used in applications with extreme mechanical, chemical, or thermal stresses, such as the inside of a wood-fired kiln or a furnace, which is subject to abrasion from wood, fluxing from ash or slag, and high temperatures. In other, less harsh situations, such as in an electric or natural gas fired kiln, more porous bricks, commonly known as \"kiln bricks\", are a better choice.[1] They are weaker, but they are much lighter and easier to form and insulate far better than dense bricks. In any case, firebricks should not spall, and their strength should hold up well during rapid temperature changes.\n",
      "\n",
      "Question: what countries did immigrants come from during the immigration\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/History_of_immigration_to_the_United_States\n",
      "Answer: Europe and later on from Asia and Latin America\n",
      "Paragraph: The history of immigration to the United States details the movement of people to the United States from the colonial era to the present. Throughout U.S. history, the country experienced successive waves of immigration, particularly from Europe and later on from Asia and Latin America. Colonial-era immigrants often repaid the cost of transoceanic transportation by becoming indentured servants in which the new employer paid the ship's captain. In the late 19th century, immigration became restricted from China and Japan. In the 1920s, restrictive immigration quotas were imposed although political refugees had special status. Numerical restrictions ended in 1965. In recent years, the largest numbers have come from Asia and Central America.\n",
      "\n",
      "Question: how many smoots in a mile\n",
      "Sorry, I could not find an answer to your question.\n",
      "Question: how tall is an indoor girls volleyball net\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Volleyball\n",
      "Answer: 2.24 m (7 ft 4+316 in)\n",
      "Paragraph: A volleyball court is 9 m × 18 m (29.5 ft × 59.1 ft), divided into equal square halves by a net with a width of one meter (39.4 in).[20] The top of the net is 2.43 m (7 ft 11+11⁄16 in) above the center of the court for men's competition, and 2.24 m (7 ft 4+3⁄16 in) for women's competition, varied for veterans and junior competitions.[3]\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Volleyball\n",
      "Answer: 8 m (26.2 ft)\n",
      "Paragraph: The minimum height clearance for indoor volleyball courts is 7 m (23.0 ft), although a clearance of 8 m (26.2 ft) is recommended.[20]\n",
      "\n",
      "Question: how many calories in a cup of white rice\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Rice\n",
      "Answer: 130\n",
      "Paragraph: Cooked white rice is 69% water, 29% carbohydrates, 2% protein, and contains negligible fat (table). In a reference serving of 100 grams (3.5 oz), cooked white rice provides 130 calories of food energy, and contains moderate levels of manganese (18% DV), with no other micronutrients in significant content (all less than 10% of the Daily Value).[37]\n",
      "In 2018, the World Health Organization strongly recommended fortifying rice with iron, and conditionally recommended fortifying it with vitamin A and with folic acid.[38]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "# Turn off all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_paragraphs_from_wikipedia(url):\n",
    "    # Send a GET request to the Wikipedia page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the paragraphs on the page\n",
    "    paragraphs = soup.find_all('p')\n",
    "    all_text = soup.get_text()\n",
    "    return (paragraphs, all_text)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def get_model():\n",
    "    # Load a pre-trained model for sentence embeddings\n",
    "    model_name = \"paraphrase-MiniLM-L6-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model\n",
    "\n",
    "def test_set():\n",
    "    questions = [\n",
    "                 \"how are glacier caves formed\", \n",
    "                 \"how much is 1 tablespoon of water ?\", \n",
    "                 \"how did anne frank die\", \n",
    "                 \"how a water pump works\", \n",
    "                 \"how old was sue lyon when she made lolita\",\n",
    "                 \"how are fire bricks made\",\n",
    "                 \"what countries did immigrants come from during the immigration\",\n",
    "                 \"how many smoots in a mile\",\n",
    "                 \"how tall is an indoor girls volleyball net\",\n",
    "                 \"how many calories in a cup of white rice\"\n",
    "                 ]\n",
    "    \n",
    "    urls = [\"https://en.wikipedia.org/wiki/Glacier_cave\",\n",
    "            \"https://en.wikipedia.org/wiki/Tablespoon\",\n",
    "            \"https://en.wikipedia.org/wiki/Anne_Frank\",\n",
    "            \"https://en.wikipedia.org/wiki/Water_pump\",\n",
    "            \"https://en.wikipedia.org/wiki/Sue_Lyon\",\n",
    "            \"https://en.wikipedia.org/wiki/Fire_brick\",\n",
    "            \"https://en.wikipedia.org/wiki/Volleyball\",\n",
    "            \"https://en.wikipedia.org/wiki/Rice\",\n",
    "            \"https://en.wikipedia.org/wiki/History_of_immigration_to_the_United_States\",\n",
    "            \"https://en.wikipedia.org/wiki/Smoot\"\n",
    "            ]\n",
    "    \n",
    "    return questions, urls\n",
    "\n",
    "def get_relevant_paragraphs(model, question, url):\n",
    "    paragraphs, text = get_paragraphs_from_wikipedia(url) \n",
    "    similarity_scores = []\n",
    "    relevant_paragraphs = []\n",
    "    question_embedding = model.encode(question, convert_to_tensor=False)\n",
    "    paragraph_embeddings = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_embedding = model.encode(paragraph.text, convert_to_tensor=False)\n",
    "        paragraph_embeddings.append(paragraph_embedding)\n",
    "                \n",
    "    # Calculate cosine similarity scores using NumPy\n",
    "    similarity_scores = cosine_similarity([question_embedding], paragraph_embeddings)\n",
    "    \n",
    "    # Filter and sort paragraphs based on similarity score\n",
    "    for i, score in enumerate(similarity_scores[0]):\n",
    "        relevant_paragraphs.append((paragraphs[i], score))\n",
    "\n",
    "    # Sort relevant paragraphs by similarity score in descending order\n",
    "    relevant_paragraphs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return relevant_paragraphs\n",
    "    \n",
    "# Define a function that generates an answer based on the question and URL\n",
    "def generate_answer(question, relevant_paragraphs, url, threshold):\n",
    "    responses = []\n",
    "    text2text_generator = pipeline(\"text2text-generation\", model=\"t5-base\") # For generating Answer text\n",
    "\n",
    "    for paragraph, score in relevant_paragraphs:\n",
    "        if (score > threshold): # TODO: Needs calibration\n",
    "            answer = text2text_generator(f\"question: {question}? context: {paragraph.text}\")\n",
    "            response = (paragraph.text, answer[0]['generated_text'], url)\n",
    "            responses.append(response)\n",
    "\n",
    "    return responses[:2] # Return upto 2 responses\n",
    "\n",
    "def print_answer(responses):\n",
    "    if (len(responses) == 0):\n",
    "        print(\"Sorry, I could not find an answer to your question.\")\n",
    "    if (len(responses) > 1):\n",
    "        print(f\"There are {len(responses)} answers to your question.\")\n",
    "    \n",
    "    for response in responses:\n",
    "        print(f\"Source Wiki Page: {response[2][0]}\")\n",
    "        print(f\"Answer: {response[1]}\")\n",
    "        print(f\"Paragraph: {response[0]}\")\n",
    "\n",
    "# Find, filter, and sort paragraphs by similarity score\n",
    "def get_relevant_url(model, question, urls):\n",
    "    relevant_urls = []\n",
    "    text_embeddings = []\n",
    "    question_embedding = model.encode(question, convert_to_tensor=False)\n",
    "    for url in urls:\n",
    "        paragraphs, text = get_paragraphs_from_wikipedia(url) \n",
    "        # Encode the question and paragraphs\n",
    "        text_embedding = model.encode(text, convert_to_tensor=False)\n",
    "        text_embeddings.append(text_embedding)\n",
    "\n",
    "    # Calculate cosine similarity scores using NumPy\n",
    "    similarity_scores = cosine_similarity([question_embedding], text_embeddings)\n",
    "    \n",
    "    # Filter and sort paragraphs based on similarity score\n",
    "    for i, score in enumerate(similarity_scores[0]):\n",
    "        relevant_urls.append((urls[i], score))\n",
    "\n",
    "    # Sort relevant paragraphs by similarity score in descending order\n",
    "    relevant_urls.sort(key=lambda x: x[1], reverse=True)\n",
    "    return relevant_urls[0]\n",
    "\n",
    "threshold = 0.7\n",
    "questions, urls = test_set()\n",
    "model = get_model()\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    most_relevant_url = get_relevant_url(model, question, urls)\n",
    "    relevant_paragraphs = get_relevant_paragraphs(model, question, most_relevant_url[0])\n",
    "    answers = generate_answer(question, relevant_paragraphs, most_relevant_url, threshold)\n",
    "    print_answer(answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output we get for our test set:\n",
    "\n",
    "```\n",
    "Question: how are glacier caves formed\n",
    "There are 2 answers to your question.\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
    "Answer: by water running through or under the glacier\n",
    "Paragraph: Most glacier caves are started by water running through or under the glacier. This water often originates on the glacier's surface through melting, entering the ice at a moulin and exiting at the glacier's snout at base level. Heat transfer from the water can cause sufficient melting to create an air-filled cavity, sometimes aided by solifluction. Air movement can then assist enlargement through melting in summer and sublimation in winter.\n",
    "\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
    "Answer: geothermal heat from volcanic vents or hotsprings beneath the ice\n",
    "Paragraph: Some glacier caves are formed by geothermal heat from volcanic vents or hotsprings beneath the ice.  An extreme example is the Kverkfjöll glacier cave in the Vatnajökull glacier in Iceland, measured in the 1980s at 2.8 kilometres (1.7 mi) long with a vertical range of 525 metres (1,722 ft).\n",
    "\n",
    "Question: how much is 1 tablespoon of water ?\n",
    "There are 2 answers to your question.\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
    "Answer: three teaspoons\n",
    "Paragraph: In most places, except Australia, one tablespoon equals three teaspoons—and one US tablespoon is 14.8 ml (0.50 US fl oz; 0.52 imp fl oz) or 15 ml (0.51 US fl oz; 0.53 imp fl oz).\n",
    "\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
    "Answer: 15 ml (0.51 US fl oz)\n",
    "Paragraph: In nutrition labeling in the U.S. and the U.K., a tablespoon is defined as 15 ml (0.51 US fl oz).[7] In Australia, the definition of the tablespoon is 20 ml (0.70 imp fl oz).[citation needed]\n",
    "\n",
    "Question: how did anne frank die\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Anne_Frank\n",
    "Answer: a typhus epidemic\n",
    "Paragraph: Anne Frank died at the Bergen-Belsen concentration camp in February or March 1945. The specific cause is unknown; however, there is evidence to suggest that she died from a typhus epidemic that spread through the camp, killing 17,000 prisoners.[98] Gena Turgel, a survivor of Bergen-Belsen, knew Anne at the camp. In 2015, she told the British newspaper The Sun: \"Her bed was around the corner from me. She was delirious, terrible, burning up.\" She said she had brought Frank water to wash.[99] Turgel, who worked in the camp hospital, said that the epidemic took a terrible toll on the inmates: \"The people were dying like flies—in the hundreds. Reports used to come in—500 people who died. Three hundred? We said, 'Thank God, only 300.'\"[99] Other diseases, including typhoid fever, were rampant.[100]\n",
    "\n",
    "Question: how a water pump works\n",
    "There are 2 answers to your question.\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
    "Answer: irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor\n",
    "Paragraph: Pumps are used throughout society for a variety of purposes.  Early applications includes the use of  the windmill or watermill to pump water.  Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.\n",
    "\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
    "Answer: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the\n",
    "Paragraph: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.\n",
    "\n",
    "Question: how old was sue lyon when she made lolita\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Sue_Lyon\n",
    "Answer: 12\n",
    "Paragraph: Although Vladimir Nabokov originally thought that Sue Lyon was the right selection to play Lolita, years later Nabokov said that the ideal Lolita would have been Catherine Demongeot, a young French actress who had played the child Zazie in Louis Malle's Zazie in the Metro (1960). The tomboyish Demongeot was four years younger than Lyon.[12]\n",
    "\n",
    "Question: how are fire bricks made\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Fire_brick\n",
    "Answer: ceramic material\n",
    "Paragraph: A fire brick, firebrick, fireclay brick, or refractory brick is a block of ceramic material used in lining furnaces, kilns, fireboxes, and fireplaces.  A refractory brick is built primarily to withstand high temperature, but will also usually have a low thermal conductivity for greater energy efficiency. Usually dense fire bricks are used in applications with extreme mechanical, chemical, or thermal stresses, such as the inside of a wood-fired kiln or a furnace, which is subject to abrasion from wood, fluxing from ash or slag, and high temperatures. In other, less harsh situations, such as in an electric or natural gas fired kiln, more porous bricks, commonly known as \"kiln bricks\", are a better choice.[1] They are weaker, but they are much lighter and easier to form and insulate far better than dense bricks. In any case, firebricks should not spall, and their strength should hold up well during rapid temperature changes.\n",
    "\n",
    "Question: what countries did immigrants come from during the immigration\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/History_of_immigration_to_the_United_States\n",
    "Answer: Europe and later on from Asia and Latin America\n",
    "Paragraph: The history of immigration to the United States details the movement of people to the United States from the colonial era to the present. Throughout U.S. history, the country experienced successive waves of immigration, particularly from Europe and later on from Asia and Latin America. Colonial-era immigrants often repaid the cost of transoceanic transportation by becoming indentured servants in which the new employer paid the ship's captain. In the late 19th century, immigration became restricted from China and Japan. In the 1920s, restrictive immigration quotas were imposed although political refugees had special status. Numerical restrictions ended in 1965. In recent years, the largest numbers have come from Asia and Central America.\n",
    "\n",
    "Question: how many smoots in a mile\n",
    "Sorry, I could not find an answer to your question.\n",
    "\n",
    "\n",
    "Question: how tall is an indoor girls volleyball net\n",
    "There are 2 answers to your question.\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Volleyball\n",
    "Answer: 2.24 m (7 ft 4+316 in)\n",
    "Paragraph: A volleyball court is 9 m × 18 m (29.5 ft × 59.1 ft), divided into equal square halves by a net with a width of one meter (39.4 in).[20] The top of the net is 2.43 m (7 ft 11+11⁄16 in) above the center of the court for men's competition, and 2.24 m (7 ft 4+3⁄16 in) for women's competition, varied for veterans and junior competitions.[3]\n",
    "\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Volleyball\n",
    "Answer: 8 m (26.2 ft)\n",
    "Paragraph: The minimum height clearance for indoor volleyball courts is 7 m (23.0 ft), although a clearance of 8 m (26.2 ft) is recommended.[20]\n",
    "\n",
    "Question: how many calories in a cup of white rice\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Rice\n",
    "Answer: 130\n",
    "Paragraph: Cooked white rice is 69% water, 29% carbohydrates, 2% protein, and contains negligible fat (table). In a reference serving of 100 grams (3.5 oz), cooked white rice provides 130 calories of food energy, and contains moderate levels of manganese (18% DV), with no other micronutrients in significant content (all less than 10% of the Daily Value).[37]\n",
    "In 2018, the World Health Organization strongly recommended fortifying rice with iron, and conditionally recommended fortifying it with vitamin A and with folic acid.[38]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation about the solution and Results**\n",
    "\n",
    "Let us first do a Visual Evaluation and in the next section, let's discuss the formal Evaluation.\n",
    "\n",
    "**Good**<br>\n",
    "Upon the visual inspection, we find that for most of the questions, the models have picked the perfect url, the perfect paragraphs and also a reasonable answer.\n",
    "\n",
    "**Shortcomings**<br>\n",
    "1. For the question, how a water pump works, the answer is not correct.\n",
    "2. For the question, how old was sue lyon when she made lolita, the answer is not correct.\n",
    "3. For the question, how are fire bricks made, the answer is partial where it specifies the material from which it is made but does not really answer the question satisfactorily.\n",
    "3. The answers are not well expressed as sentences. In some cases, the answer  I planned to improve the text-generation of the answers. However, due to shortage of time, I am leaving it to next steps\n",
    "4. For the question, **how many smoots in a mile**, it could not determine the answer. The answer is present in the wiki page but not in a direct form. Since this is a language model, it does not have an ability to **calculate** or do conversions. This model should hence be combined with some math tools to be able to extract relevant information and compute the answer instead of just looking for extraction of the answer from the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "I would have two kinds of metrics for this problem.\n",
    "1. End-End metrics\n",
    "2. Metrics for each step.\n",
    "\n",
    "**Metrics for each step**:\n",
    "There are 3 steps in this algorithm\n",
    "1. Relevance of the Url to the question\n",
    "2. Relevance of the paragrah to the question\n",
    "3. Answer Generation\n",
    "\n",
    "**Relevance metric**\n",
    "\n",
    "For both the 'Relevance' steps, it is important that the model picks the top entities among all that is provided. In addition to the **ordering** of the entities (url or the paragraph), it is also important to keep only the relevant ones and prune away the noise.\n",
    "\n",
    "The metric I would recommend for evaluation of the Ranking order is the NDCG. Normalized Discounted Cumulative Gain (NDCG) is an information retrieval metric used to evaluate the quality of ranked search results. It considers both the relevance of retrieved items and their positions in the ranking. NDCG calculates a score that ranges from 0 to 1, with higher values indicating better-ranked results, and it penalizes lower-ranked relevant items more severely, providing a more accurate measure of search result quality. Particularly I would be using NDCG@1 and NDCG@3 metrics\n",
    "\n",
    "The metric I would chose for evaluation of the correct selection of paragraphs is Precision and Recall. \n",
    "\n",
    "**Answer Generation metric**\n",
    "For Answer Generation, I would use the BLEU Score (Bilingual Evaluation Understudy Score): BLEU measures the similarity between the generated answer and one or more reference answers and also the ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation): ROUGE measures the overlap between the generated answer and reference answers in terms of n-grams (unigrams, bigrams, etc.). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for metric computation\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "#region NDCG\n",
    "# Function to compute NDCG\n",
    "def ndcg_at_k(ranked_list, k, ground_truth):\n",
    "    # Ensure k is within the bounds of the list length\n",
    "    k = min(k, len(ranked_list))\n",
    "    \n",
    "    # Compute DCG (Discounted Cumulative Gain) at k\n",
    "    dcg = 0.0\n",
    "    for i in range(k):\n",
    "        rel_i = 1 if ranked_list[i] in ground_truth else 0  # Binary relevance\n",
    "        dcg += (2 ** rel_i - 1) / np.log2(i + 2)  # +2 because of 0-based indexing\n",
    "    \n",
    "    # Compute IDCG (Ideal DCG) at k\n",
    "    ideal_ranking = sorted(ground_truth, reverse=True)\n",
    "    idcg = 0.0\n",
    "    for i in range(k):\n",
    "        rel_i = 1 if ideal_ranking[i] in ground_truth else 0\n",
    "        idcg += (2 ** rel_i - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Compute NDCG\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "    return ndcg\n",
    "\n",
    "# Compute NDCG at different values of k (e.g., k=1, k=3, k=5)\n",
    "def compute_ndcg(ranked_list, ground_truth):\n",
    "    ndcg_values = []\n",
    "    for k in [1, 3, 5]:\n",
    "        ndcg_values.append(ndcg_at_k(ranked_list, k, ground_truth))\n",
    "    return ndcg_values\n",
    "\n",
    "#endregion NDCG\n",
    "\n",
    "#region P/R/F1\n",
    "def presicion_recall_f1(relevant_docs, retrieved_docs):\n",
    "    # Compute Precision, Recall, and F1\n",
    "    precision = len(set(relevant_docs).intersection(set(retrieved_docs))) / len(retrieved_docs)\n",
    "    recall = len(set(relevant_docs).intersection(set(retrieved_docs))) / len(relevant_docs)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "#endregion P/R/F1\n",
    "\n",
    "#region BLUE\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import statistics\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "\n",
    "def compute_bleu_scores(candidate_reference_pairs):\n",
    "    bleu_scores = []\n",
    "\n",
    "    for candidate, reference in candidate_reference_pairs:\n",
    "        candidate_tokens = candidate.split()\n",
    "        reference_tokens = reference.split()\n",
    "        \n",
    "        # Compute BLEU score for each pair\n",
    "        bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    return bleu_scores\n",
    "\n",
    "def compute_rouge_scores(candidate_reference_pairs):\n",
    "    rouge_scores = []\n",
    "\n",
    "    for candidate, reference in candidate_reference_pairs:\n",
    "        # Create a ROUGE scorer\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "        # Compute ROUGE scores for each pair\n",
    "        scores = scorer.score(reference, candidate)\n",
    "\n",
    "        # Extract and append ROUGE-1 and ROUGE-L F1 scores\n",
    "        rouge1_f1 = scores[\"rouge1\"].fmeasure\n",
    "        rougeL_f1 = scores[\"rougeL\"].fmeasure\n",
    "        rouge_scores.append((rouge1_f1, rougeL_f1))\n",
    "\n",
    "    return rouge_scores\n",
    "\n",
    "# Define a function to compute BLEU and ROUGE scores\n",
    "def compute_scores(candidate_reference_pairs):\n",
    "    # Compute BLEU scores\n",
    "    bleu_scores = compute_bleu_scores(candidate_reference_pairs)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = compute_rouge_scores(candidate_reference_pairs)\n",
    "\n",
    "    # Calculate the mean BLEU score\n",
    "    mean_bleu_score = statistics.mean(bleu_scores)\n",
    "\n",
    "    # Calculate the mean ROUGE-1 and ROUGE-L F1 scores\n",
    "    rouge1_f1_scores, rougeL_f1_scores = zip(*rouge_scores)\n",
    "    mean_rouge1_f1 = statistics.mean(rouge1_f1_scores)\n",
    "    mean_rougeL_f1 = statistics.mean(rougeL_f1_scores)\n",
    "\n",
    "    # Print the mean scores\n",
    "    print(f'Mean BLEU Score: {mean_bleu_score:.2f}')\n",
    "    print(f'Mean ROUGE-1 F1 Score: {mean_rouge1_f1:.2f}')\n",
    "    print(f'Mean ROUGE-L F1 Score: {mean_rougeL_f1:.2f}')\n",
    "\n",
    "#endregion BLUE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have observed during our run with out-of-the-box models, there are some short-comings for our scenario in using them.\n",
    "There are primarily two types of models involved: Relevance models and Answer extraction models.\n",
    "\n",
    "**Relevance models** <br>\n",
    "Our current model **paraphrase-MiniLM-L6-v2** has performed quite impressively and is a good candidate to fine-tune.\n",
    "We will use SQUAD or the WikiQA dataset for this purpose. \n",
    "Since the scenario for ranking both paragraphs or the entire url is the same, the only difference being the length of the context, we can aim to fine-tune only one model instead of two.\n",
    "\n",
    "**Answer Extraction models**\n",
    "Our current model **t5-base** is a reasonably well performing model and is a good candidate to fine-tune.\n",
    "We will use SQUAD or the WikiQA dataset for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for Fine-tuning paraphrase-MiniLM-L6-v2\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#region working-version Fine-tuning \n",
    "# (using SQUAD dataset and DistilBERT)\n",
    "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
    "# answers are allowed or not).\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "#endregion working-version Fine-tuning\n",
    "\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "def generate_training_examples():\n",
    "    pass\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "\n",
    "def fine_tune_relevance(output_path):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "    train_examples = generate_training_examples()\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=1,\n",
    "          warmup_steps=100)\n",
    "    model.save('output_path\\\\fine-tuned-MiniLM-model')\n",
    "    # model = SentenceTransformer('output_path\\\\fine-tuned-model')\n",
    "\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def tokenize_function(tokenizer, examples):\n",
    "    inputs = [f\"question: {q} context: {c}\" for q, c in zip(examples['question'], examples['context'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['answers'], max_length=128, truncation=True, padding='max_length')\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def fine_tune_answer_generation(output_path):\n",
    "    model_name = \"t5-base\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    dataset = load_dataset('.\\\\WikiQA', split='train')\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained('output_path\\\\fine-tuned-T5-model')\n",
    "    tokenizer.save_pretrained('output_path\\\\fine-tuned-T5-model')\n",
    "\n",
    "    # Use this in the script at the top - Problem: Fine-tuning is taking way too long to complete\n",
    "    # model = T5ForConditionalGeneration.from_pretrained('output_path\\\\fine-tuned-T5-model')\n",
    "    # tokenizer = T5Tokenizer.from_pretrained('output_path\\\\fine-tuned-T5-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion on Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary & Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
