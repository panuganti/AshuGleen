{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "- Problem Statement\n",
    "- Given an open-source model, Execute the application\n",
    "- Discussion on the selection of the model and the algorithm\n",
    "- Discussion on Evaluation\n",
    "- Fine-tuning the model\n",
    "- Closing Remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CheatApp\n",
    "\n",
    "We are going to develop a cheating app for open domain question answering systems through a notebook. In this app, we would like to suggest users of the wikipedia page with the relevant answers for given questions. To further stretch the challenge, we would like to suggest the best paragraphs having the answers of the questions in the corresponding wikipedia page. Below are few examples - \n",
    "\n",
    "Question:  how are glacier caves formed ?\n",
    "wikipedia page - Glacier cave - Wikipedia   \n",
    "paragraph : ‘A glacier cave is a cave formed within the ice of a glacier. Glacier caves are often called ice caves, but the latter term is properly used to describe bedrock caves that contain year-round ice’ (summary of the page). \n",
    "\n",
    "Question - how much is 1 tablespoon of water ?\n",
    "wikipedia page -https://en.wikipedia.org/wiki/Tablespoon  \n",
    "paragraph is - It has multiple answers. It could like - \n",
    "‘In most places, except Australia, one tablespoon equals three teaspoons—and one US tablespoon is 14.8 ml (0.50 US fl oz; 0.52 imp fl oz) or 15 ml (0.51 US fl oz; 0.53 imp fl oz).’ \n",
    "Or\n",
    " ‘In nutrition labeling in the U.S. and the U.K., a tablespoon is defined as 15 ml (0.51 US fl oz).[7] In Australia, the definition of the tablespoon is 20 ml (0.70 imp fl oz)’ etc.\n",
    "\n",
    "Question - how did anne frank die \n",
    "wikipedia page - https://en.wikipedia.org/wiki/Anne_Frank \n",
    "Paragraph - ‘Following their arrest, the Franks were transported to concentration camps. On 1 November 1944,[2] Anne and her sister, Margot, were transferred from Auschwitz to Bergen-Belsen concentration camp, where they died (probably of typhus) a few months later. They were originally estimated by the Red Cross to have died in March, with Dutch authorities setting 31 March as the official date. Later research has suggested they died in February or early March.’\n",
    "\n",
    "Expectation\n",
    "Given this is an open problem, we don’t expect a particular level of correctness. What we are mainly looking for - how you approach and quickly prototype crappy solutions. Then you keep adding complex logic in iterations to achieve some satisfactory levels. While doing that journey, we expect that you may generate following artifacts - \n",
    "Hypothesis and motivations for choosing different modeling techniques.\n",
    "How you measured the model performance. \n",
    "Data curation, training/evaluation data generations, model performance measurements etc.\n",
    "end 2 end machine learning pipeline in python notebook including above steps.\n",
    "Also, what constraints you felt which led you not to try the things you wanted to do to solve this problem is an awesome way.\n",
    "** -  If you use an already available model/code/library from the web, we expect that you have a full understanding of motivation and why you are using it. Ex:- if you use entity linking library, we expect that you understand - pros and cons of that model. This includes - Why do you think your chosen entity linking library is good for your problem?  When do you expect your chosen model may behave poorly? \n",
    "\n",
    "Resources\n",
    "\n",
    "You are free to use open source resources including already available  annotated training data on the web. Also, free to use already trained models & libraries existing in open source. What we mainly expect is - how you approach the problems and journey.\n",
    "\n",
    "You are not allowed to use llm libraries like Langchain and LammaIndex. \n",
    "\n",
    "Wikipedia text data is available in Kaggle at - wikidata-text\n",
    "Also added sample open questions and expected answers - wikipedia_question_similar_answer.tsv . The answers added here are not exact wikipedia graphs, but it may be super helpful for your modeling techniques. \n",
    "\n",
    "Other open source resources that can be used are - https://paperswithcode.com/dataset/wikiqa (questions in wikipedia_question_similar_answer.tsv is taken from this data set).\n",
    "\n",
    "\n",
    "\n",
    "Notes\n",
    "Please create a loom video explaining all solutions/approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Solution\n",
    "The problem corresponds to the Question-Answering problem of the NLP domain.\n",
    "Inputs: Query, a set of wiki-urls\n",
    "Output: Answer with citations (Let's limit to 2 (configurable) for brevity)\n",
    "\n",
    "Algorithm:\n",
    "Step 1: Find the most relevant url. <br>\n",
    "There can be multiple url's satisfying a question. However, for simplicity, I assumed only one url has the relevant answer.\n",
    "Please note that there is nothing in the algorithm that breaks if there are more than 1 url that can contain the answer\n",
    "The algorithm we use for this case is the cosine similarity score to rank the urls as per relevance to the question.\n",
    "\n",
    "For the computation of these embeddings, we will use the model hugging-face's **paraphrase-MiniLM-L6-v2**.\n",
    "We compute the embedding for the question and also the embedding for the entire text in the wiki url.\n",
    "Then, we sort the urls based on the cosine similarity between the question and the text of the wiki url.\n",
    "\n",
    "**Why we chose this model**\n",
    "Wiki text can be particularly long context since the answer can be present anywhere in the text. \n",
    "**paraphrase-MiniLM-L6-v2** is a model that can handle a faily long text in the context. I have experimented with other models like **distilbert-base-uncased**.\n",
    "They are however unable to generate a good embedding for a long context. \n",
    "The quality of this model can also be observed in the tests that I have executed to test this algorithm. See the output of the next Code cell.\n",
    "\n",
    "**pros**\n",
    "1. Good quality\n",
    "2. Long Context\n",
    "\n",
    "**cons**\n",
    "1. Higher latency\n",
    "2. Not perfect. It needs to be tuned further.\n",
    "\n",
    "Step 2: Find the most relevant paragraph within the url. <br>\n",
    "We first split the text into separate paragraphs.\n",
    "We compute the embeddings for each of these paragrahs and the top-2 most relevant paragraphs are chosen again based on the cosine similarity.\n",
    "For this step too, we choose the same model. In this step, ideally, we can chose a much lightweight model including **distilbert-base-uncased**.\n",
    "The performance of **distilbert-base-uncased** model is also reasonably satisfactory for such smaller context. Even though, the **paraphrase-MiniLM-L6-v2** performs slightly better.\n",
    "\n",
    "Step 3: From the most relevant paragraphs, generate the answer. <br>\n",
    "Now that we found the most relevant paragraphs that might contain the answer, it is important to extract the answer.\n",
    "For this, we use HuggingFace's **text2text-generation** pipeline with **t5-base**. The model performs particularly well especially in short contexts.\n",
    "There are better models than **t5-base** but this model is sufficient for the length of context we are supplying. In addition, it is of lower latency.\n",
    "\n",
    "The reason why Step 3 is important is that we need to determine if there is present at all or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how are glacier caves formed\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
      "Answer: by water running through or under the glacier\n",
      "Paragraph: Most glacier caves are started by water running through or under the glacier. This water often originates on the glacier's surface through melting, entering the ice at a moulin and exiting at the glacier's snout at base level. Heat transfer from the water can cause sufficient melting to create an air-filled cavity, sometimes aided by solifluction. Air movement can then assist enlargement through melting in summer and sublimation in winter.\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
      "Answer: geothermal heat from volcanic vents or hotsprings beneath the ice\n",
      "Paragraph: Some glacier caves are formed by geothermal heat from volcanic vents or hotsprings beneath the ice.  An extreme example is the Kverkfjöll glacier cave in the Vatnajökull glacier in Iceland, measured in the 1980s at 2.8 kilometres (1.7 mi) long with a vertical range of 525 metres (1,722 ft).\n",
      "\n",
      "Question: how much is 1 tablespoon of water ?\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
      "Answer: three teaspoons\n",
      "Paragraph: In most places, except Australia, one tablespoon equals three teaspoons—and one US tablespoon is 14.8 ml (0.50 US fl oz; 0.52 imp fl oz) or 15 ml (0.51 US fl oz; 0.53 imp fl oz).\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
      "Answer: 15 ml (0.51 US fl oz)\n",
      "Paragraph: In nutrition labeling in the U.S. and the U.K., a tablespoon is defined as 15 ml (0.51 US fl oz).[7] In Australia, the definition of the tablespoon is 20 ml (0.70 imp fl oz).[citation needed]\n",
      "\n",
      "Question: how did anne frank die\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Anne_Frank\n",
      "Answer: a typhus epidemic\n",
      "Paragraph: Anne Frank died at the Bergen-Belsen concentration camp in February or March 1945. The specific cause is unknown; however, there is evidence to suggest that she died from a typhus epidemic that spread through the camp, killing 17,000 prisoners.[98] Gena Turgel, a survivor of Bergen-Belsen, knew Anne at the camp. In 2015, she told the British newspaper The Sun: \"Her bed was around the corner from me. She was delirious, terrible, burning up.\" She said she had brought Frank water to wash.[99] Turgel, who worked in the camp hospital, said that the epidemic took a terrible toll on the inmates: \"The people were dying like flies—in the hundreds. Reports used to come in—500 people who died. Three hundred? We said, 'Thank God, only 300.'\"[99] Other diseases, including typhoid fever, were rampant.[100]\n",
      "\n",
      "Question: how a water pump works\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
      "Answer: irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor\n",
      "Paragraph: Pumps are used throughout society for a variety of purposes.  Early applications includes the use of  the windmill or watermill to pump water.  Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
      "Answer: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the\n",
      "Paragraph: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.\n",
      "\n",
      "Question: how old was sue lyon when she made lolita\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Sue_Lyon\n",
      "Answer: 12\n",
      "Paragraph: Although Vladimir Nabokov originally thought that Sue Lyon was the right selection to play Lolita, years later Nabokov said that the ideal Lolita would have been Catherine Demongeot, a young French actress who had played the child Zazie in Louis Malle's Zazie in the Metro (1960). The tomboyish Demongeot was four years younger than Lyon.[12]\n",
      "\n",
      "Question: how are fire bricks made\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Fire_brick\n",
      "Answer: ceramic material\n",
      "Paragraph: A fire brick, firebrick, fireclay brick, or refractory brick is a block of ceramic material used in lining furnaces, kilns, fireboxes, and fireplaces.  A refractory brick is built primarily to withstand high temperature, but will also usually have a low thermal conductivity for greater energy efficiency. Usually dense fire bricks are used in applications with extreme mechanical, chemical, or thermal stresses, such as the inside of a wood-fired kiln or a furnace, which is subject to abrasion from wood, fluxing from ash or slag, and high temperatures. In other, less harsh situations, such as in an electric or natural gas fired kiln, more porous bricks, commonly known as \"kiln bricks\", are a better choice.[1] They are weaker, but they are much lighter and easier to form and insulate far better than dense bricks. In any case, firebricks should not spall, and their strength should hold up well during rapid temperature changes.\n",
      "\n",
      "Question: what countries did immigrants come from during the immigration\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/History_of_immigration_to_the_United_States\n",
      "Answer: Europe and later on from Asia and Latin America\n",
      "Paragraph: The history of immigration to the United States details the movement of people to the United States from the colonial era to the present. Throughout U.S. history, the country experienced successive waves of immigration, particularly from Europe and later on from Asia and Latin America. Colonial-era immigrants often repaid the cost of transoceanic transportation by becoming indentured servants in which the new employer paid the ship's captain. In the late 19th century, immigration became restricted from China and Japan. In the 1920s, restrictive immigration quotas were imposed although political refugees had special status. Numerical restrictions ended in 1965. In recent years, the largest numbers have come from Asia and Central America.\n",
      "\n",
      "Question: how many smoots in a mile\n",
      "Sorry, I could not find an answer to your question.\n",
      "Question: how tall is an indoor girls volleyball net\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Volleyball\n",
      "Answer: 2.24 m (7 ft 4+316 in)\n",
      "Paragraph: A volleyball court is 9 m × 18 m (29.5 ft × 59.1 ft), divided into equal square halves by a net with a width of one meter (39.4 in).[20] The top of the net is 2.43 m (7 ft 11+11⁄16 in) above the center of the court for men's competition, and 2.24 m (7 ft 4+3⁄16 in) for women's competition, varied for veterans and junior competitions.[3]\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Volleyball\n",
      "Answer: 8 m (26.2 ft)\n",
      "Paragraph: The minimum height clearance for indoor volleyball courts is 7 m (23.0 ft), although a clearance of 8 m (26.2 ft) is recommended.[20]\n",
      "\n",
      "Question: how many calories in a cup of white rice\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Rice\n",
      "Answer: 130\n",
      "Paragraph: Cooked white rice is 69% water, 29% carbohydrates, 2% protein, and contains negligible fat (table). In a reference serving of 100 grams (3.5 oz), cooked white rice provides 130 calories of food energy, and contains moderate levels of manganese (18% DV), with no other micronutrients in significant content (all less than 10% of the Daily Value).[37]\n",
      "In 2018, the World Health Organization strongly recommended fortifying rice with iron, and conditionally recommended fortifying it with vitamin A and with folic acid.[38]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "# Turn off all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_paragraphs_from_wikipedia(url):\n",
    "    # Send a GET request to the Wikipedia page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the paragraphs on the page\n",
    "    paragraphs = soup.find_all('p')\n",
    "    all_text = soup.get_text()\n",
    "    return (paragraphs, all_text)\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "def get_model():\n",
    "    # Load a pre-trained model for sentence embeddings\n",
    "    model_name = \"paraphrase-MiniLM-L6-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model\n",
    "\n",
    "def test_set():\n",
    "    questions = [\n",
    "                 \"how are glacier caves formed\", \n",
    "                 \"how much is 1 tablespoon of water ?\", \n",
    "                 \"how did anne frank die\", \n",
    "                 \"how a water pump works\", \n",
    "                 \"how old was sue lyon when she made lolita\",\n",
    "                 \"how are fire bricks made\",\n",
    "                 \"what countries did immigrants come from during the immigration\",\n",
    "                 \"how many smoots in a mile\",\n",
    "                 \"how tall is an indoor girls volleyball net\",\n",
    "                 \"how many calories in a cup of white rice\"\n",
    "                 ]\n",
    "    \n",
    "    urls = [\"https://en.wikipedia.org/wiki/Glacier_cave\",\n",
    "            \"https://en.wikipedia.org/wiki/Tablespoon\",\n",
    "            \"https://en.wikipedia.org/wiki/Anne_Frank\",\n",
    "            \"https://en.wikipedia.org/wiki/Water_pump\",\n",
    "            \"https://en.wikipedia.org/wiki/Sue_Lyon\",\n",
    "            \"https://en.wikipedia.org/wiki/Fire_brick\",\n",
    "            \"https://en.wikipedia.org/wiki/Volleyball\",\n",
    "            \"https://en.wikipedia.org/wiki/Rice\",\n",
    "            \"https://en.wikipedia.org/wiki/History_of_immigration_to_the_United_States\",\n",
    "            \"https://en.wikipedia.org/wiki/Smoot\"\n",
    "            ]\n",
    "    \n",
    "    return questions, urls\n",
    "\n",
    "def get_relevant_paragraphs(model, question, url):\n",
    "    paragraphs, text = get_paragraphs_from_wikipedia(url) \n",
    "    similarity_scores = []\n",
    "    relevant_paragraphs = []\n",
    "    question_embedding = model.encode(question, convert_to_tensor=False)\n",
    "    paragraph_embeddings = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_embedding = model.encode(paragraph.text, convert_to_tensor=False)\n",
    "        paragraph_embeddings.append(paragraph_embedding)\n",
    "                \n",
    "    # Calculate cosine similarity scores using NumPy\n",
    "    similarity_scores = cosine_similarity([question_embedding], paragraph_embeddings)\n",
    "    \n",
    "    # Filter and sort paragraphs based on similarity score\n",
    "    for i, score in enumerate(similarity_scores[0]):\n",
    "        relevant_paragraphs.append((paragraphs[i], score))\n",
    "\n",
    "    # Sort relevant paragraphs by similarity score in descending order\n",
    "    relevant_paragraphs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return relevant_paragraphs\n",
    "    \n",
    "# Define a function that generates an answer based on the question and URL\n",
    "def generate_answer(question, relevant_paragraphs, url, threshold):\n",
    "    responses = []\n",
    "    text2text_generator = pipeline(\"text2text-generation\", model=\"t5-base\") # For generating Answer text\n",
    "\n",
    "    for paragraph, score in relevant_paragraphs:\n",
    "        if (score > threshold): # TODO: Needs calibration\n",
    "            answer = text2text_generator(f\"question: {question}? context: {paragraph.text}\")\n",
    "            response = (paragraph.text, answer[0]['generated_text'], url)\n",
    "            responses.append(response)\n",
    "\n",
    "    return responses[:2] # Return upto 2 responses\n",
    "\n",
    "def print_answer(responses):\n",
    "    if (len(responses) == 0):\n",
    "        print(\"Sorry, I could not find an answer to your question.\")\n",
    "    if (len(responses) > 1):\n",
    "        print(f\"There are {len(responses)} answers to your question.\")\n",
    "    \n",
    "    for response in responses:\n",
    "        print(f\"Source Wiki Page: {response[2][0]}\")\n",
    "        print(f\"Answer: {response[1]}\")\n",
    "        print(f\"Paragraph: {response[0]}\")\n",
    "\n",
    "# Find, filter, and sort paragraphs by similarity score\n",
    "def get_relevant_url(model, question, urls):\n",
    "    relevant_urls = []\n",
    "    text_embeddings = []\n",
    "    question_embedding = model.encode(question, convert_to_tensor=False)\n",
    "    for url in urls:\n",
    "        paragraphs, text = get_paragraphs_from_wikipedia(url) \n",
    "        # Encode the question and paragraphs\n",
    "        text_embedding = model.encode(text, convert_to_tensor=False)\n",
    "        text_embeddings.append(text_embedding)\n",
    "\n",
    "    # Calculate cosine similarity scores using NumPy\n",
    "    similarity_scores = cosine_similarity([question_embedding], text_embeddings)\n",
    "    \n",
    "    # Filter and sort paragraphs based on similarity score\n",
    "    for i, score in enumerate(similarity_scores[0]):\n",
    "        relevant_urls.append((urls[i], score))\n",
    "\n",
    "    # Sort relevant paragraphs by similarity score in descending order\n",
    "    relevant_urls.sort(key=lambda x: x[1], reverse=True)\n",
    "    return relevant_urls[0]\n",
    "\n",
    "threshold = 0.7\n",
    "questions, urls = test_set()\n",
    "model = get_model()\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    most_relevant_url = get_relevant_url(model, question, urls)\n",
    "    relevant_paragraphs = get_relevant_paragraphs(model, question, most_relevant_url[0])\n",
    "    answers = generate_answer(question, relevant_paragraphs, most_relevant_url, threshold)\n",
    "    print_answer(answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the output we get for our test set:\n",
    "\n",
    "```\n",
    "Question: how are glacier caves formed\n",
    "There are 2 answers to your question.\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
    "Answer: by water running through or under the glacier\n",
    "Paragraph: Most glacier caves are started by water running through or under the glacier. This water often originates on the glacier's surface through melting, entering the ice at a moulin and exiting at the glacier's snout at base level. Heat transfer from the water can cause sufficient melting to create an air-filled cavity, sometimes aided by solifluction. Air movement can then assist enlargement through melting in summer and sublimation in winter.\n",
    "\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
    "Answer: geothermal heat from volcanic vents or hotsprings beneath the ice\n",
    "Paragraph: Some glacier caves are formed by geothermal heat from volcanic vents or hotsprings beneath the ice.  An extreme example is the Kverkfjöll glacier cave in the Vatnajökull glacier in Iceland, measured in the 1980s at 2.8 kilometres (1.7 mi) long with a vertical range of 525 metres (1,722 ft).\n",
    "\n",
    "Question: how much is 1 tablespoon of water ?\n",
    "There are 2 answers to your question.\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
    "Answer: three teaspoons\n",
    "Paragraph: In most places, except Australia, one tablespoon equals three teaspoons—and one US tablespoon is 14.8 ml (0.50 US fl oz; 0.52 imp fl oz) or 15 ml (0.51 US fl oz; 0.53 imp fl oz).\n",
    "\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
    "Answer: 15 ml (0.51 US fl oz)\n",
    "Paragraph: In nutrition labeling in the U.S. and the U.K., a tablespoon is defined as 15 ml (0.51 US fl oz).[7] In Australia, the definition of the tablespoon is 20 ml (0.70 imp fl oz).[citation needed]\n",
    "\n",
    "Question: how did anne frank die\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Anne_Frank\n",
    "Answer: a typhus epidemic\n",
    "Paragraph: Anne Frank died at the Bergen-Belsen concentration camp in February or March 1945. The specific cause is unknown; however, there is evidence to suggest that she died from a typhus epidemic that spread through the camp, killing 17,000 prisoners.[98] Gena Turgel, a survivor of Bergen-Belsen, knew Anne at the camp. In 2015, she told the British newspaper The Sun: \"Her bed was around the corner from me. She was delirious, terrible, burning up.\" She said she had brought Frank water to wash.[99] Turgel, who worked in the camp hospital, said that the epidemic took a terrible toll on the inmates: \"The people were dying like flies—in the hundreds. Reports used to come in—500 people who died. Three hundred? We said, 'Thank God, only 300.'\"[99] Other diseases, including typhoid fever, were rampant.[100]\n",
    "\n",
    "Question: how a water pump works\n",
    "There are 2 answers to your question.\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
    "Answer: irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor\n",
    "Paragraph: Pumps are used throughout society for a variety of purposes.  Early applications includes the use of  the windmill or watermill to pump water.  Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.\n",
    "\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
    "Answer: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the\n",
    "Paragraph: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.\n",
    "\n",
    "Question: how old was sue lyon when she made lolita\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Sue_Lyon\n",
    "Answer: 12\n",
    "Paragraph: Although Vladimir Nabokov originally thought that Sue Lyon was the right selection to play Lolita, years later Nabokov said that the ideal Lolita would have been Catherine Demongeot, a young French actress who had played the child Zazie in Louis Malle's Zazie in the Metro (1960). The tomboyish Demongeot was four years younger than Lyon.[12]\n",
    "\n",
    "Question: how are fire bricks made\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Fire_brick\n",
    "Answer: ceramic material\n",
    "Paragraph: A fire brick, firebrick, fireclay brick, or refractory brick is a block of ceramic material used in lining furnaces, kilns, fireboxes, and fireplaces.  A refractory brick is built primarily to withstand high temperature, but will also usually have a low thermal conductivity for greater energy efficiency. Usually dense fire bricks are used in applications with extreme mechanical, chemical, or thermal stresses, such as the inside of a wood-fired kiln or a furnace, which is subject to abrasion from wood, fluxing from ash or slag, and high temperatures. In other, less harsh situations, such as in an electric or natural gas fired kiln, more porous bricks, commonly known as \"kiln bricks\", are a better choice.[1] They are weaker, but they are much lighter and easier to form and insulate far better than dense bricks. In any case, firebricks should not spall, and their strength should hold up well during rapid temperature changes.\n",
    "\n",
    "Question: what countries did immigrants come from during the immigration\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/History_of_immigration_to_the_United_States\n",
    "Answer: Europe and later on from Asia and Latin America\n",
    "Paragraph: The history of immigration to the United States details the movement of people to the United States from the colonial era to the present. Throughout U.S. history, the country experienced successive waves of immigration, particularly from Europe and later on from Asia and Latin America. Colonial-era immigrants often repaid the cost of transoceanic transportation by becoming indentured servants in which the new employer paid the ship's captain. In the late 19th century, immigration became restricted from China and Japan. In the 1920s, restrictive immigration quotas were imposed although political refugees had special status. Numerical restrictions ended in 1965. In recent years, the largest numbers have come from Asia and Central America.\n",
    "\n",
    "Question: how many smoots in a mile\n",
    "Sorry, I could not find an answer to your question.\n",
    "\n",
    "\n",
    "Question: how tall is an indoor girls volleyball net\n",
    "There are 2 answers to your question.\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Volleyball\n",
    "Answer: 2.24 m (7 ft 4+316 in)\n",
    "Paragraph: A volleyball court is 9 m × 18 m (29.5 ft × 59.1 ft), divided into equal square halves by a net with a width of one meter (39.4 in).[20] The top of the net is 2.43 m (7 ft 11+11⁄16 in) above the center of the court for men's competition, and 2.24 m (7 ft 4+3⁄16 in) for women's competition, varied for veterans and junior competitions.[3]\n",
    "\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Volleyball\n",
    "Answer: 8 m (26.2 ft)\n",
    "Paragraph: The minimum height clearance for indoor volleyball courts is 7 m (23.0 ft), although a clearance of 8 m (26.2 ft) is recommended.[20]\n",
    "\n",
    "Question: how many calories in a cup of white rice\n",
    "Source Wiki Page: https://en.wikipedia.org/wiki/Rice\n",
    "Answer: 130\n",
    "Paragraph: Cooked white rice is 69% water, 29% carbohydrates, 2% protein, and contains negligible fat (table). In a reference serving of 100 grams (3.5 oz), cooked white rice provides 130 calories of food energy, and contains moderate levels of manganese (18% DV), with no other micronutrients in significant content (all less than 10% of the Daily Value).[37]\n",
    "In 2018, the World Health Organization strongly recommended fortifying rice with iron, and conditionally recommended fortifying it with vitamin A and with folic acid.[38]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation about the solution and Results**\n",
    "\n",
    "Let us first do a Visual Evaluation and in the next section, let's discuss the formal Evaluation.\n",
    "\n",
    "**Good**<br>\n",
    "Upon the visual inspection, we find that for most of the questions, the models have picked the perfect url, the perfect paragraphs and also a reasonable answer.\n",
    "\n",
    "**Shortcomings**<br>\n",
    "1. For the question, how a water pump works, the answer is not correct.\n",
    "2. For the question, how old was sue lyon when she made lolita, the answer is not correct.\n",
    "3. For the question, how are fire bricks made, the answer is partial where it specifies the material from which it is made but does not really answer the question satisfactorily.\n",
    "3. The answers are not well expressed as sentences. In some cases, the answer  I planned to improve the text-generation of the answers. However, due to shortage of time, I am leaving it to next steps\n",
    "4. For the question, **how many smoots in a mile**, it could not determine the answer. The answer is present in the wiki page but not in a direct form. Since this is a language model, it does not have an ability to **calculate** or do conversions. This model should hence be combined with some math tools to be able to extract relevant information and compute the answer instead of just looking for extraction of the answer from the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "I would have two kinds of metrics for this problem.\n",
    "1. End-End metrics\n",
    "2. Metrics for each step.\n",
    "\n",
    "**Metrics for each step**:\n",
    "There are 3 steps in this algorithm\n",
    "1. Relevance of the Url to the question\n",
    "2. Relevance of the paragrah to the question\n",
    "3. Answer Generation\n",
    "\n",
    "**Relevance metric**\n",
    "\n",
    "For both the 'Relevance' steps, it is important that the model picks the top entities among all that is provided. In addition to the **ordering** of the entities (url or the paragraph), it is also important to keep only the relevant ones and prune away the noise.\n",
    "\n",
    "The metric I would recommend for evaluation of the Ranking order is the NDCG. Normalized Discounted Cumulative Gain (NDCG) is an information retrieval metric used to evaluate the quality of ranked search results. It considers both the relevance of retrieved items and their positions in the ranking. NDCG calculates a score that ranges from 0 to 1, with higher values indicating better-ranked results, and it penalizes lower-ranked relevant items more severely, providing a more accurate measure of search result quality. Particularly I would be using NDCG@1 and NDCG@3 metrics\n",
    "\n",
    "The metric I would chose for evaluation of the correct selection of paragraphs is Precision and Recall. \n",
    "\n",
    "**Answer Generation metric**\n",
    "For Answer Generation, I would use the BLEU Score (Bilingual Evaluation Understudy Score): BLEU measures the similarity between the generated answer and one or more reference answers and also the ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation): ROUGE measures the overlap between the generated answer and reference answers in terms of n-grams (unigrams, bigrams, etc.). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for metric computation\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "#region NDCG\n",
    "# Function to compute NDCG\n",
    "def ndcg_at_k(ranked_list, k, ground_truth):\n",
    "    # Ensure k is within the bounds of the list length\n",
    "    k = min(k, len(ranked_list))\n",
    "    \n",
    "    # Compute DCG (Discounted Cumulative Gain) at k\n",
    "    dcg = 0.0\n",
    "    for i in range(k):\n",
    "        rel_i = 1 if ranked_list[i] in ground_truth else 0  # Binary relevance\n",
    "        dcg += (2 ** rel_i - 1) / np.log2(i + 2)  # +2 because of 0-based indexing\n",
    "    \n",
    "    # Compute IDCG (Ideal DCG) at k\n",
    "    ideal_ranking = sorted(ground_truth, reverse=True)\n",
    "    idcg = 0.0\n",
    "    for i in range(k):\n",
    "        rel_i = 1 if ideal_ranking[i] in ground_truth else 0\n",
    "        idcg += (2 ** rel_i - 1) / np.log2(i + 2)\n",
    "    \n",
    "    # Compute NDCG\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
    "    return ndcg\n",
    "\n",
    "# Compute NDCG at different values of k (e.g., k=1, k=3, k=5)\n",
    "def compute_ndcg(ranked_list, ground_truth):\n",
    "    ndcg_values = []\n",
    "    for k in [1, 3, 5]:\n",
    "        ndcg_values.append(ndcg_at_k(ranked_list, k, ground_truth))\n",
    "    return ndcg_values\n",
    "\n",
    "#endregion NDCG\n",
    "\n",
    "#region P/R/F1\n",
    "def presicion_recall_f1(relevant_docs, retrieved_docs):\n",
    "    # Compute Precision, Recall, and F1\n",
    "    precision = len(set(relevant_docs).intersection(set(retrieved_docs))) / len(retrieved_docs)\n",
    "    recall = len(set(relevant_docs).intersection(set(retrieved_docs))) / len(relevant_docs)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "#endregion P/R/F1\n",
    "\n",
    "#region BLUE\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import statistics\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "\n",
    "def compute_bleu_scores(candidate_reference_pairs):\n",
    "    bleu_scores = []\n",
    "\n",
    "    for candidate, reference in candidate_reference_pairs:\n",
    "        candidate_tokens = candidate.split()\n",
    "        reference_tokens = reference.split()\n",
    "        \n",
    "        # Compute BLEU score for each pair\n",
    "        bleu_score = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "        bleu_scores.append(bleu_score)\n",
    "\n",
    "    return bleu_scores\n",
    "\n",
    "def compute_rouge_scores(candidate_reference_pairs):\n",
    "    rouge_scores = []\n",
    "\n",
    "    for candidate, reference in candidate_reference_pairs:\n",
    "        # Create a ROUGE scorer\n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "        # Compute ROUGE scores for each pair\n",
    "        scores = scorer.score(reference, candidate)\n",
    "\n",
    "        # Extract and append ROUGE-1 and ROUGE-L F1 scores\n",
    "        rouge1_f1 = scores[\"rouge1\"].fmeasure\n",
    "        rougeL_f1 = scores[\"rougeL\"].fmeasure\n",
    "        rouge_scores.append((rouge1_f1, rougeL_f1))\n",
    "\n",
    "    return rouge_scores\n",
    "\n",
    "# Define a function to compute BLEU and ROUGE scores\n",
    "def compute_scores(candidate_reference_pairs):\n",
    "    # Compute BLEU scores\n",
    "    bleu_scores = compute_bleu_scores(candidate_reference_pairs)\n",
    "\n",
    "    # Compute ROUGE scores\n",
    "    rouge_scores = compute_rouge_scores(candidate_reference_pairs)\n",
    "\n",
    "    # Calculate the mean BLEU score\n",
    "    mean_bleu_score = statistics.mean(bleu_scores)\n",
    "\n",
    "    # Calculate the mean ROUGE-1 and ROUGE-L F1 scores\n",
    "    rouge1_f1_scores, rougeL_f1_scores = zip(*rouge_scores)\n",
    "    mean_rouge1_f1 = statistics.mean(rouge1_f1_scores)\n",
    "    mean_rougeL_f1 = statistics.mean(rougeL_f1_scores)\n",
    "\n",
    "    # Print the mean scores\n",
    "    print(f'Mean BLEU Score: {mean_bleu_score:.2f}')\n",
    "    print(f'Mean ROUGE-1 F1 Score: {mean_rouge1_f1:.2f}')\n",
    "    print(f'Mean ROUGE-L F1 Score: {mean_rougeL_f1:.2f}')\n",
    "\n",
    "#endregion BLUE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have observed during our run with out-of-the-box models, there are some short-comings for our scenario in using them.\n",
    "There are primarily two types of models involved: Relevance models and Answer extraction models.\n",
    "\n",
    "**Relevance models** <br>\n",
    "Our current model **paraphrase-MiniLM-L6-v2** has performed quite impressively and is a good candidate to fine-tune.\n",
    "We will use SQUAD or the WikiQA dataset for this purpose. \n",
    "Since the scenario for ranking both paragraphs or the entire url is the same, the only difference being the length of the context, we can aim to fine-tune only one model instead of two.\n",
    "\n",
    "**Answer Extraction models**\n",
    "Our current model **t5-base** is a reasonably well performing model and is a good candidate to fine-tune.\n",
    "We will use SQUAD or the WikiQA dataset for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572edd9ecb0c0d14000f1639</td>\n",
       "      <td>Transistor</td>\n",
       "      <td>From November 17, 1947 to December 23, 1947, John Bardeen and Walter Brattain at AT&amp;T's Bell Labs in the United States performed experiments and observed that when two gold point contacts were applied to a crystal of germanium, a signal was produced with the output power greater than the input. Solid State Physics Group leader William Shockley saw the potential in this, and over the next few months worked to greatly expand the knowledge of semiconductors. The term transistor was coined by John R. Pierce as a contraction of the term transresistance. According to Lillian Hoddeson and Vicki Daitch, authors of a biography of John Bardeen, Shockley had proposed that Bell Labs' first patent for a transistor should be based on the field-effect and that he be named as the inventor. Having unearthed Lilienfeld’s patents that went into obscurity years earlier, lawyers at Bell Labs advised against Shockley's proposal because the idea of a field-effect transistor that used an electric field as a \"grid\" was not new. Instead, what Bardeen, Brattain, and Shockley invented in 1947 was the first point-contact transistor. In acknowledgement of this accomplishment, Shockley, Bardeen, and Brattain were jointly awarded the 1956 Nobel Prize in Physics \"for their researches on semiconductors and their discovery of the transistor effect.\"</td>\n",
       "      <td>Who came up with the term transistor?</td>\n",
       "      <td>{'text': ['John R. Pierce'], 'answer_start': [494]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5731cb260fdd8d15006c6533</td>\n",
       "      <td>Pacific_War</td>\n",
       "      <td>Japan sponsored several puppet governments, one of which was headed by Wang Jingwei. However, its policies of brutality toward the Chinese population, of not yielding any real power to these regimes, and of supporting several rival governments failed to make any of them a viable alternative to the Nationalist government led by Chiang Kai-shek. Conflicts between Chinese communist and nationalist forces vying for territory control behind enemy lines culminated in a major armed clash in January 1941, effectively ending their co-operation.</td>\n",
       "      <td>What government did Chiang Kai-shek lead?</td>\n",
       "      <td>{'text': ['Nationalist government'], 'answer_start': [299]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57325f030fdd8d15006c6a4e</td>\n",
       "      <td>Protestantism</td>\n",
       "      <td>Contrary to how the Protestant Reformers were often characterized, the concept of a catholic or universal Church was not brushed aside during the Protestant Reformation. On the contrary, the visible unity of the catholic or universal church was seen by the Protestant reformers as an important and essential doctrine of the Reformation. The Magisterial reformers, such as Martin Luther, John Calvin, and Huldrych Zwingli, believed that they were reforming the Roman Catholic Church, which they viewed as having become corrupted. Each of them took very seriously the charges of schism and innovation, denying these charges and maintaining that it was the Roman Catholic Church that had left them. In order to justify their departure from the Roman Catholic Church, Protestants often posited a new argument, saying that there was no real visible Church with divine authority, only a spiritual, invisible, and hidden church—this notion began in the early days of the Protestant Reformation.</td>\n",
       "      <td>When did the idea of a hidden church begin?</td>\n",
       "      <td>{'text': ['in the early days of the Protestant Reformation'], 'answer_start': [939]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57305bb7069b5314008320b4</td>\n",
       "      <td>Translation</td>\n",
       "      <td>Generally, the greater the contact and exchange that have existed between two languages, or between those languages and a third one, the greater is the ratio of metaphrase to paraphrase that may be used in translating among them. However, due to shifts in ecological niches of words, a common etymology is sometimes misleading as a guide to current meaning in one or the other language. For example, the English actual should not be confused with the cognate French actuel (\"present\", \"current\"), the Polish aktualny (\"present\", \"current,\" \"topical,\" \"timely,\" \"feasible\"), the Swedish aktuell (\"topical\", \"presently of importance\"), the Russian актуальный (\"urgent\", \"topical\") or the Dutch actueel.</td>\n",
       "      <td>How would you convey that something is \"presently of importance\" in Swedish?</td>\n",
       "      <td>{'text': ['aktuell'], 'answer_start': [586]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56f81fbba6d7ea1400e173e2</td>\n",
       "      <td>Szlachta</td>\n",
       "      <td>In 1422 King Władysław II Jagiełło by the Privilege of Czerwińsk (Polish: \"przywilej czerwiński\") established the inviolability of nobles' property (their estates could not be confiscated except upon a court verdict) and ceded some jurisdiction over fiscal policy to the Royal Council (later, the Senat of Poland), including the right to mint coinage.</td>\n",
       "      <td>When did the established right for inviolability of nobles property?</td>\n",
       "      <td>{'text': ['1422'], 'answer_start': [3]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>573007ceb2c2fd1400568765</td>\n",
       "      <td>Charleston,_South_Carolina</td>\n",
       "      <td>The highest temperature recorded within city limits was 104 °F (40 °C), on June 2, 1985, and June 24, 1944, and the lowest was 7 °F (−14 °C) on February 14, 1899, although at the airport, where official records are kept, the historical range is 105 °F (41 °C) on August 1, 1999 down to 6 °F (−14 °C) on January 21, 1985. Hurricanes are a major threat to the area during the summer and early fall, with several severe hurricanes hitting the area – most notably Hurricane Hugo on September 21, 1989 (a category 4 storm). Dewpoint in the summer ranges from 67.8 to 71.4 °F (20 to 22 °C).</td>\n",
       "      <td>What day did Charleston's airport hit the coldest day on record?</td>\n",
       "      <td>{'text': ['January 21, 1985'], 'answer_start': [303]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>57277ff9dd62a815002e9ed7</td>\n",
       "      <td>Political_party</td>\n",
       "      <td>While there is some international commonality in the way political parties are recognized, and in how they operate, there are often many differences, and some are significant. Many political parties have an ideological core, but some do not, and many represent very different ideologies than they did when first founded. In democracies, political parties are elected by the electorate to run a government. Many countries have numerous powerful political parties, such as Germany and India and some nations have one-party systems, such as China. The United States is a two-party system, with its two most powerful parties being the Democratic Party and the Republican Party.</td>\n",
       "      <td>Name a nation that has a one-party political system.</td>\n",
       "      <td>{'text': ['China'], 'answer_start': [538]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56f8d0389e9bad19000a0582</td>\n",
       "      <td>Guinea-Bissau</td>\n",
       "      <td>Guinea-Bissau is a republic. In the past, the government had been highly centralized. Multi-party governance was not established until mid-1991. The president is the head of state and the prime minister is the head of government. Since 1974, no president has successfully served a full five-year term.</td>\n",
       "      <td>Who is the head of government?</td>\n",
       "      <td>{'text': ['prime minister'], 'answer_start': [188]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5731939a497a881900249064</td>\n",
       "      <td>Steven_Spielberg</td>\n",
       "      <td>His next theatrical release in that same year was the World War II film Saving Private Ryan, about a group of U.S. soldiers led by Capt. Miller (Tom Hanks) sent to bring home a paratrooper whose three older brothers were killed in the same twenty-four hours, June 5–6, of the Normandy landing. The film was a huge box office success, grossing over $481 million worldwide and was the biggest film of the year at the North American box office (worldwide it made second place after Michael Bay's Armageddon). Spielberg won his second Academy Award for his direction. The film's graphic, realistic depiction of combat violence influenced later war films such as Black Hawk Down and Enemy at the Gates. The film was also the first major hit for DreamWorks, which co-produced the film with Paramount Pictures (as such, it was Spielberg's first release from the latter that was not part of the Indiana Jones series). Later, Spielberg and Tom Hanks produced a TV mini-series based on Stephen Ambrose's book Band of Brothers. The ten-part HBO mini-series follows Easy Company of the 101st Airborne Division's 506th Parachute Infantry Regiment. The series won a number of awards at the Golden Globes and the Emmys.</td>\n",
       "      <td>Which film beat 'Saving Private Ryan' worldwide?</td>\n",
       "      <td>{'text': ['Armageddon'], 'answer_start': [493]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>572f8f15a23a5019007fc74b</td>\n",
       "      <td>Bacteria</td>\n",
       "      <td>There are typically 40 million bacterial cells in a gram of soil and a million bacterial cells in a millilitre of fresh water. There are approximately 5×1030 bacteria on Earth, forming a biomass which exceeds that of all plants and animals. Bacteria are vital in recycling nutrients, with many of the stages in nutrient cycles dependent on these organisms, such as the fixation of nitrogen from the atmosphere and putrefaction. In the biological communities surrounding hydrothermal vents and cold seeps, bacteria provide the nutrients needed to sustain life by converting dissolved compounds, such as hydrogen sulphide and methane, to energy. On 17 March 2013, researchers reported data that suggested bacterial life forms thrive in the Mariana Trench, which with a depth of up to 11 kilometres is the deepest part of the Earth's oceans. Other researchers reported related studies that microbes thrive inside rocks up to 580 metres below the sea floor under 2.6 kilometres of ocean off the coast of the northwestern United States. According to one of the researchers, \"You can find microbes everywhere — they're extremely adaptable to conditions, and survive wherever they are.\"</td>\n",
       "      <td>How does bacteria help to sustain life in hydrothermal vents and cold seeps?</td>\n",
       "      <td>{'text': ['by converting dissolved compounds'], 'answer_start': [559]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Code for Fine-tuning paraphrase-MiniLM-L6-v2\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#region working-version Fine-tuning \n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# (using SQUAD dataset and DistilBERT)\n",
    "# This flag is the difference between SQUAD v1 or 2 (if you're using another dataset, it indicates if impossible\n",
    "# answers are allowed or not).\n",
    "#squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "datasets = load_dataset(\"squad\")\n",
    "# The datasets object itself is DatasetDict, which contains one key for the training, validation and test set.\n",
    "\n",
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Print a random sample of 5 elements from the training set:\n",
    "# We can see the answers are indicated by their start position in the text (here at character 515) and their full text, which is a substring of the context as we mentioned above.\n",
    "# To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset (automatically decoding the labels in passing).\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(datasets[\"train\"])\n",
    "\n",
    "# Preprocessing the training data\n",
    "# Before we can feed those texts to our model, we need to preprocess them. \n",
    "# This is done by a Transformers Tokenizer\n",
    "# To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
    "\n",
    "# we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "# we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import transformers    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")\n",
    "max_length = 384 # The maximum length of a feature (question and context)\n",
    "doc_stride = 128 # The authorized overlap between two part of the context when splitting it is needed.\n",
    "\n",
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "example = datasets[\"train\"][i]\n",
    "\n",
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "[len(x) for x in tokenized_example[\"input_ids\"]]\n",
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])\n",
    "\n",
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# Start token index of the current span in the text.\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# End token index of the current span in the text.\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # Move the token_start_index and token_end_index to the two ends of the answer.\n",
    "    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"The answer is not in this feature.\")\n",
    "\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "#endregion working-version Fine-tuning\n",
    "\n",
    "\n",
    "from sentence_transformers import InputExample\n",
    "def generate_training_examples():\n",
    "    pass\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import losses\n",
    "\n",
    "def fine_tune_relevance(output_path):\n",
    "    model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')\n",
    "    train_examples = generate_training_examples()\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=1,\n",
    "          warmup_steps=100)\n",
    "    model.save('output_path\\\\fine-tuned-MiniLM-model')\n",
    "    # model = SentenceTransformer('output_path\\\\fine-tuned-model')\n",
    "\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def tokenize_function(tokenizer, examples):\n",
    "    inputs = [f\"question: {q} context: {c}\" for q, c in zip(examples['question'], examples['context'])]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['answers'], max_length=128, truncation=True, padding='max_length')\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "def fine_tune_answer_generation(output_path):\n",
    "    model_name = \"t5-base\"\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    dataset = load_dataset('.\\\\WikiQA', split='train')\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained('output_path\\\\fine-tuned-T5-model')\n",
    "    tokenizer.save_pretrained('output_path\\\\fine-tuned-T5-model')\n",
    "\n",
    "    # Use this in the script at the top - Problem: Fine-tuning is taking way too long to complete\n",
    "    # model = T5ForConditionalGeneration.from_pretrained('output_path\\\\fine-tuned-T5-model')\n",
    "    # tokenizer = T5Tokenizer.from_pretrained('output_path\\\\fine-tuned-T5-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning Evaluation\n",
    "\n",
    "The model itself predicts logits for the start and en position of our answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the model is a dict-like object that contains the loss (since we provided labels), the start and end logits. We won't need the loss for our predictions, let's have a look a the logits:\n",
    "This will work great in a lot of cases, but what if this prediction gives us something impossible: the start position could be greater than the end position, or point to a span of text in the question instead of the answer. In that case, we might want to look at the second best prediction to see if it gives a possible answer and select that instead.\n",
    "\n",
    "However, picking the second best answer is not as easy as picking the best one: is it the second best index in the start logits with the best index in the end logits? Or the best index in the start logits with the second best index in the end logits? And if that second best answer is not possible either, it gets even trickier for the third best answer.\n",
    "\n",
    "To classify our answers, we will use the score obtained by adding the start and end logits. We won't try to order all the possible answers and limit ourselves to with a hyper-parameter we call n_best_size. We'll pick the best indices in the start and end logits and gather all the answers this predicts. After checking if each one is valid, we will sort them by their score and keep the best one. Here is how we would do this on the first feature in the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_best_size = 20\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\" # We need to find a way to get back the original substring corresponding to the answer in the context\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can sort the valid_answers according to their score and only keep the best one. The only point left is how to check a given span is inside the context (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
    "\n",
    "the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
    "the offset mapping that will give us a map from token indices to character positions in the context.\n",
    "That's why we will re-process the validation set with the following function, slightly different from prepare_train_features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now refine the test we had before: since we set None in the offset mappings when it corresponds to a part of the question, it's easy to check if an answer is fully inside the context. We also eliminate very long answers from our considerations (with an hyper-parameter we can tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
    "# an example index\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# Gather the indices the best start/end logits:\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "        # to part of the input_ids that are not in the context.\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # We need to refine that test to check the answer is inside the context\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion on Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I have attempted the Fine-Tuning. However, the fine-tuning was taking a very long time in the fine-tuning step.\n",
    "\n",
    "Discussion on the choices made:\n",
    "We have opted to chose the models that are giving us the best performance and attempted fine-tuning on that model.\n",
    "We have chose SQUAD dataset for the fine-tuning. Reason for this choice: Our focus is on getting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary & Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I inspected the problem, the ask seems to be along the lines of building the Question-Answering pipeline using the standard datasets. \n",
    "Since the problem seemed to be on the standard datsets, I wanted to first use out-of-box models that many people have trained on such datasets.\n",
    "\n",
    "Since the focus was on the process and not on evaluating the best possible performance, I also wanted to pick models that can be run on CPU instead of GPU, for quicker iteration on my local laptop (instead of google colab) and also so that I have a use-case where I can possibly demonstrate the training loop to improve the model. \n",
    "\n",
    "I have demonstrated how I modeled the solution to the problem. \n",
    "I have also reasoned why I picked those models for the specific components of the solution.\n",
    "\n",
    "I also discussed the metrics to be used for evaluating both the end-end metrics as well as for evaluating individual components of the solution.\n",
    "\n",
    "I have run a test set of 10 questions and 10 urls to demonstrate the effectiveness of the solution that I proposed.\n",
    "I have also discussed the short-comings of the solution and the need to fine-tune.\n",
    "\n",
    "For the fine-tuning loop, I have picked the SQUAD dataset. I added examples from the examples that were performing not so great to this dataset.\n",
    "I have completed the data processing steps. I have attemped fine-tuning. However, the fine-tuning was taking too much time. There aren't errors but I had to abort the run to make progress on other components. I have attemped using a different model and a smaller dataset. But, model trained using this compromise was terrible in performance.\n",
    "\n",
    "I have also discussed the evaluation of the fine-tuning process. and the metrics chosen for the same.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
