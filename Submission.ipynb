{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "\n",
    "- Problem Statement\n",
    "- Given an open-source model, Execute the application\n",
    "- Discussion on the selection of the model and the algorithm\n",
    "- Discussion on Evaluation\n",
    "- Fine-tuning the model\n",
    "- Closing Remarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CheatApp\n",
    "\n",
    "We are going to develop a cheating app for open domain question answering systems through a notebook. In this app, we would like to suggest users of the wikipedia page with the relevant answers for given questions. To further stretch the challenge, we would like to suggest the best paragraphs having the answers of the questions in the corresponding wikipedia page. Below are few examples - \n",
    "\n",
    "Question:  how are glacier caves formed ?\n",
    "wikipedia page - Glacier cave - Wikipedia   \n",
    "paragraph : ‘A glacier cave is a cave formed within the ice of a glacier. Glacier caves are often called ice caves, but the latter term is properly used to describe bedrock caves that contain year-round ice’ (summary of the page). \n",
    "\n",
    "Question - how much is 1 tablespoon of water ?\n",
    "wikipedia page -https://en.wikipedia.org/wiki/Tablespoon  \n",
    "paragraph is - It has multiple answers. It could like - \n",
    "‘In most places, except Australia, one tablespoon equals three teaspoons—and one US tablespoon is 14.8 ml (0.50 US fl oz; 0.52 imp fl oz) or 15 ml (0.51 US fl oz; 0.53 imp fl oz).’ \n",
    "Or\n",
    " ‘In nutrition labeling in the U.S. and the U.K., a tablespoon is defined as 15 ml (0.51 US fl oz).[7] In Australia, the definition of the tablespoon is 20 ml (0.70 imp fl oz)’ etc.\n",
    "\n",
    "Question - how did anne frank die \n",
    "wikipedia page - https://en.wikipedia.org/wiki/Anne_Frank \n",
    "Paragraph - ‘Following their arrest, the Franks were transported to concentration camps. On 1 November 1944,[2] Anne and her sister, Margot, were transferred from Auschwitz to Bergen-Belsen concentration camp, where they died (probably of typhus) a few months later. They were originally estimated by the Red Cross to have died in March, with Dutch authorities setting 31 March as the official date. Later research has suggested they died in February or early March.’\n",
    "\n",
    "Expectation\n",
    "Given this is an open problem, we don’t expect a particular level of correctness. What we are mainly looking for - how you approach and quickly prototype crappy solutions. Then you keep adding complex logic in iterations to achieve some satisfactory levels. While doing that journey, we expect that you may generate following artifacts - \n",
    "Hypothesis and motivations for choosing different modeling techniques.\n",
    "How you measured the model performance. \n",
    "Data curation, training/evaluation data generations, model performance measurements etc.\n",
    "end 2 end machine learning pipeline in python notebook including above steps.\n",
    "Also, what constraints you felt which led you not to try the things you wanted to do to solve this problem is an awesome way.\n",
    "** -  If you use an already available model/code/library from the web, we expect that you have a full understanding of motivation and why you are using it. Ex:- if you use entity linking library, we expect that you understand - pros and cons of that model. This includes - Why do you think your chosen entity linking library is good for your problem?  When do you expect your chosen model may behave poorly? \n",
    "\n",
    "Resources\n",
    "\n",
    "You are free to use open source resources including already available  annotated training data on the web. Also, free to use already trained models & libraries existing in open source. What we mainly expect is - how you approach the problems and journey.\n",
    "\n",
    "You are not allowed to use llm libraries like Langchain and LammaIndex. \n",
    "\n",
    "Wikipedia text data is available in Kaggle at - wikidata-text\n",
    "Also added sample open questions and expected answers - wikipedia_question_similar_answer.tsv . The answers added here are not exact wikipedia graphs, but it may be super helpful for your modeling techniques. \n",
    "\n",
    "Other open source resources that can be used are - https://paperswithcode.com/dataset/wikiqa (questions in wikipedia_question_similar_answer.tsv is taken from this data set).\n",
    "\n",
    "\n",
    "\n",
    "Notes\n",
    "Please create a loom video explaining all solutions/approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Solution\n",
    "The problem corresponds to the Question-Answering problem of the NLP domain.\n",
    "Inputs: Query, a set of wiki-urls\n",
    "Output: Answer with citations (Let's limit to 2 (configurable) for brevity)\n",
    "\n",
    "Preparation:\n",
    "1. For each url, we fetch the text content and first compute the embeddings for each url.\n",
    "2. Also, for each paragraph in the url, we fetch the text content and compute the embeddings.\n",
    "\n",
    "For the computation of these embeddings, we will use the model hugging-face's **distilbert-base-uncased**.\n",
    "\n",
    "Querying:\n",
    "1. We compute the embedding of the Question.\n",
    "\n",
    "Searching for Answer:\n",
    "1. \n",
    "\n",
    "Reasons for using **distilbert-base-uncased**\n",
    "Note: My claim is not that this is the best model.\n",
    "Distilbert is nearly 3 year old model and was known to have one of the best performance for Question Answering tasks until 2 years ago. DistilBert is a distilled version of the BERT model. \n",
    "\n",
    "\n",
    "**Pros**:\n",
    "1. Small and hence can run on PC\n",
    "2. Fast to iterate\n",
    "3. It is trained on raw-text without human-labeling-bias. Hence, it is a good model for fine-tuning task.\n",
    "4. The model is very appropriate for fine-tuning that uses entire sentence like Question-Answering. \n",
    "5. It is not very 'generative' and hence is more appropriate for giving importance to facts.\n",
    "\n",
    "**Cons**\n",
    "Being a small model, it's performance is far from perfect, but, is sufficient for most 'simple' (i.e retrieve sentence where answer exists etc.) kind of Question-Answering tasks.\n",
    "\n",
    "Summary: We chose this model because it is well suited to demonstrate the effect of fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: how are glacier caves formed\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
      "Answer: by water running through or under the glacier\n",
      "Paragraph: Most glacier caves are started by water running through or under the glacier. This water often originates on the glacier's surface through melting, entering the ice at a moulin and exiting at the glacier's snout at base level. Heat transfer from the water can cause sufficient melting to create an air-filled cavity, sometimes aided by solifluction. Air movement can then assist enlargement through melting in summer and sublimation in winter.\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Glacier_cave\n",
      "Answer: geothermal heat from volcanic vents or hotsprings beneath the ice\n",
      "Paragraph: Some glacier caves are formed by geothermal heat from volcanic vents or hotsprings beneath the ice.  An extreme example is the Kverkfjöll glacier cave in the Vatnajökull glacier in Iceland, measured in the 1980s at 2.8 kilometres (1.7 mi) long with a vertical range of 525 metres (1,722 ft).\n",
      "\n",
      "Question: how much is 1 tablespoon of water ?\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
      "Answer: three teaspoons\n",
      "Paragraph: In most places, except Australia, one tablespoon equals three teaspoons—and one US tablespoon is 14.8 ml (0.50 US fl oz; 0.52 imp fl oz) or 15 ml (0.51 US fl oz; 0.53 imp fl oz).\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Tablespoon\n",
      "Answer: 15 ml (0.51 US fl oz)\n",
      "Paragraph: In nutrition labeling in the U.S. and the U.K., a tablespoon is defined as 15 ml (0.51 US fl oz).[7] In Australia, the definition of the tablespoon is 20 ml (0.70 imp fl oz).[citation needed]\n",
      "\n",
      "Question: how did anne frank die\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Anne_Frank\n",
      "Answer: a typhus epidemic\n",
      "Paragraph: Anne Frank died at the Bergen-Belsen concentration camp in February or March 1945. The specific cause is unknown; however, there is evidence to suggest that she died from a typhus epidemic that spread through the camp, killing 17,000 prisoners.[98] Gena Turgel, a survivor of Bergen-Belsen, knew Anne at the camp. In 2015, she told the British newspaper The Sun: \"Her bed was around the corner from me. She was delirious, terrible, burning up.\" She said she had brought Frank water to wash.[99] Turgel, who worked in the camp hospital, said that the epidemic took a terrible toll on the inmates: \"The people were dying like flies—in the hundreds. Reports used to come in—500 people who died. Three hundred? We said, 'Thank God, only 300.'\"[99] Other diseases, including typhoid fever, were rampant.[100]\n",
      "\n",
      "Question: how a water pump works\n",
      "There are 2 answers to your question.\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
      "Answer: irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor\n",
      "Paragraph: Pumps are used throughout society for a variety of purposes.  Early applications includes the use of  the windmill or watermill to pump water.  Today, the pump is used for irrigation, water supply, gasoline supply, air conditioning systems, refrigeration (usually called a compressor), chemical movement, sewage movement, flood control, marine services, etc.\n",
      "\n",
      "Source Wiki Page: https://en.wikipedia.org/wiki/Water_pump\n",
      "Answer: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the\n",
      "Paragraph: Mechanical pumps may be submerged in the fluid they are pumping or be placed external to the fluid.\n",
      "\n",
      "Question: how old was sue lyon when she made lolita\n"
     ]
    }
   ],
   "source": [
    "# Solution\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "# Turn off all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Set a similarity score threshold -- based on test data\n",
    "threshold = 0.7\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_paragraphs_from_wikipedia(url):\n",
    "    # Send a GET request to the Wikipedia page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the paragraphs on the page\n",
    "    paragraphs = soup.find_all('p')\n",
    "    all_text = soup.get_text()\n",
    "    return (paragraphs, all_text)\n",
    "\n",
    "from transformers import pipeline, DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "def get_answer(model, tokenizer, question, context):\n",
    "    # Create a Question Answering pipeline\n",
    "    qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Perform question answering\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "    # Extract the answer\n",
    "    answer = result[\"answer\"]\n",
    "    return answer\n",
    "\n",
    "def get_similarity_score(model, tokenizer, question, context):\n",
    "    # Tokenize the context and question\n",
    "    inputs = tokenizer(context, question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Get the embeddings for the tokens\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        context_embeddings = outputs.last_hidden_state[:, 0]  # Context embeddings\n",
    "        question_embeddings = outputs.last_hidden_state[:, 1]  # Question embeddings\n",
    "\n",
    "    # Compute the cosine similarity score between context and question embeddings\n",
    "    similarity_score = torch.cosine_similarity(context_embeddings, question_embeddings).item()\n",
    "    return similarity_score\n",
    "\n",
    "# Find, filter, and sort paragraphs by similarity score\n",
    "def filter_and_sort_paragraphs(model, question, paragraphs, threshold):\n",
    "    relevant_paragraphs = []\n",
    "\n",
    "    question_embedding = model.encode(question, convert_to_tensor=False)\n",
    "    # Encode the question and paragraphs\n",
    "    non_empty_paragraphs = [p.text for p in paragraphs if p.text.strip() != \"\"]\n",
    "    paragraph_embeddings = model.encode(non_empty_paragraphs, convert_to_tensor=False)\n",
    "\n",
    "    # Calculate cosine similarity scores using NumPy\n",
    "    similarity_scores = cosine_similarity([question_embedding], paragraph_embeddings)\n",
    "    \n",
    "    # Filter and sort paragraphs based on similarity score\n",
    "    for i, score in enumerate(similarity_scores[0]):\n",
    "        relevant_paragraphs.append((paragraphs[i], score))\n",
    "\n",
    "    # Sort relevant paragraphs by similarity score in descending order\n",
    "    relevant_paragraphs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return relevant_paragraphs\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from transformers import pipeline\n",
    "def get_model():\n",
    "    # Load a pre-trained model for sentence embeddings\n",
    "    model_name = \"paraphrase-MiniLM-L6-v2\"\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model\n",
    "\n",
    "def get_urls_embeddings(urls):\n",
    "    url_embedding = {}\n",
    "    for url in urls:\n",
    "        (paragraphs, text) = get_paragraphs_from_wikipedia(urls)\n",
    "        embedding = model.encode(text, convert_to_tensor=False)\n",
    "        url_embedding[url] = embedding\n",
    "    return url_embedding\n",
    "\n",
    "def test_set():\n",
    "    questions = [\n",
    "                 \"how are glacier caves formed\", \n",
    "                 \"how much is 1 tablespoon of water ?\", \n",
    "                 \"how did anne frank die\", \n",
    "                 \"how a water pump works\", \n",
    "                 \"how old was sue lyon when she made lolita\",\n",
    "                 \"how are fire bricks made\",\n",
    "                 \"what countries did immigrants come from during the immigration\",\n",
    "                 \"how many smoots in a mile\"   \n",
    "                 \"how tall is an indoor girls volleyball net\",\n",
    "                 \"how many calories in a cup of white rice\",\n",
    "                 ]\n",
    "    \n",
    "    urls = [\"https://en.wikipedia.org/wiki/Glacier_cave\",\n",
    "            \"https://en.wikipedia.org/wiki/Tablespoon\",\n",
    "            \"https://en.wikipedia.org/wiki/Anne_Frank\",\n",
    "            \"https://en.wikipedia.org/wiki/Water_pump\",\n",
    "            \"https://en.wikipedia.org/wiki/Sue_Lyon\",\n",
    "            \"https://en.wikipedia.org/wiki/Fire_brick\",\n",
    "            \"https://en.wikipedia.org/wiki/Volleyball\",\n",
    "            \"https://en.wikipedia.org/wiki/Rice\",\n",
    "            \"https://en.wikipedia.org/wiki/History_of_immigration_to_the_United_States\",\n",
    "            \"https://en.wikipedia.org/wiki/Smoot\"\n",
    "            ]\n",
    "    \n",
    "    return questions, urls\n",
    "\n",
    "def get_relevant_paragraphs(model, question, url):\n",
    "    paragraphs, text = get_paragraphs_from_wikipedia(url) \n",
    "    similarity_scores = []\n",
    "    relevant_paragraphs = []\n",
    "    question_embedding = model.encode(question, convert_to_tensor=False)\n",
    "    paragraph_embeddings = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_embedding = model.encode(paragraph.text, convert_to_tensor=False)\n",
    "        paragraph_embeddings.append(paragraph_embedding)\n",
    "                \n",
    "    # Calculate cosine similarity scores using NumPy\n",
    "    similarity_scores = cosine_similarity([question_embedding], paragraph_embeddings)\n",
    "    \n",
    "    # Filter and sort paragraphs based on similarity score\n",
    "    for i, score in enumerate(similarity_scores[0]):\n",
    "        relevant_paragraphs.append((paragraphs[i], score))\n",
    "\n",
    "    # Sort relevant paragraphs by similarity score in descending order\n",
    "    relevant_paragraphs.sort(key=lambda x: x[1], reverse=True)\n",
    "    return relevant_paragraphs\n",
    "    \n",
    "# Define a function that generates an answer based on the question and URL\n",
    "def generate_answer(question, relevant_paragraphs, url, threshold):\n",
    "    responses = []\n",
    "    text2text_generator = pipeline(\"text2text-generation\", model=\"t5-base\") # For generating Answer text\n",
    "\n",
    "    for paragraph, score in relevant_paragraphs:\n",
    "        if (score > threshold): # TODO: Needs calibration\n",
    "            answer = text2text_generator(f\"question: {question}? context: {paragraph.text}\")\n",
    "            response = (paragraph.text, answer[0]['generated_text'], url)\n",
    "            responses.append(response)\n",
    "\n",
    "    return responses[:2] # Return upto 2 responses\n",
    "\n",
    "def print_answer(responses):\n",
    "    if (len(responses) == 0):\n",
    "        print(\"Sorry, I could not find an answer to your question.\")\n",
    "    if (len(responses) > 1):\n",
    "        print(f\"There are {len(responses)} answers to your question.\")\n",
    "    \n",
    "    for response in responses:\n",
    "        print(f\"Source Wiki Page: {response[2][0]}\")\n",
    "        print(f\"Answer: {response[1]}\")\n",
    "        print(f\"Paragraph: {response[0]}\")\n",
    "\n",
    "# Find, filter, and sort paragraphs by similarity score\n",
    "def get_relevant_url(model, question, urls):\n",
    "    relevant_urls = []\n",
    "    text_embeddings = []\n",
    "    question_embedding = model.encode(question, convert_to_tensor=False)\n",
    "    for url in urls:\n",
    "        paragraphs, text = get_paragraphs_from_wikipedia(url) \n",
    "        # Encode the question and paragraphs\n",
    "        text_embedding = model.encode(text, convert_to_tensor=False)\n",
    "        text_embeddings.append(text_embedding)\n",
    "\n",
    "    # Calculate cosine similarity scores using NumPy\n",
    "    similarity_scores = cosine_similarity([question_embedding], text_embeddings)\n",
    "    \n",
    "    # Filter and sort paragraphs based on similarity score\n",
    "    for i, score in enumerate(similarity_scores[0]):\n",
    "        relevant_urls.append((urls[i], score))\n",
    "\n",
    "    # Sort relevant paragraphs by similarity score in descending order\n",
    "    relevant_urls.sort(key=lambda x: x[1], reverse=True)\n",
    "    return relevant_urls[0]\n",
    "\n",
    "questions, urls = test_set()\n",
    "model = get_model()\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    most_relevant_url = get_relevant_url(model, question, urls)\n",
    "    relevant_paragraphs = get_relevant_paragraphs(model, question, most_relevant_url[0])\n",
    "    answers = generate_answer(question, relevant_paragraphs, most_relevant_url, threshold)\n",
    "    print_answer(answers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation about the solution and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion on Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion on Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary & Closing Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
